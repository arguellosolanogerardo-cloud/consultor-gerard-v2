# üö® DIAGN√ìSTICO URGENTE: LLM Responde `[]` Vac√≠o

## PROBLEMA IDENTIFICADO

**S√≠ntoma:** El LLM (Gemini 2.5 Pro) devuelve literalmente `[]` (array vac√≠o) incluso cuando:
- ‚úÖ FAISS encuentra 75 documentos relevantes
- ‚úÖ El retriever env√≠a 21,636 caracteres de contexto
- ‚úÖ El contexto incluye "linaje ra", "linaje bis", etc.

**Causa ra√≠z:**  Con `chunk_size=300`, la informaci√≥n est√° TAN FRAGMENTADA que el LLM NO puede armar una respuesta coherente.

---

## EVIDENCIA

### Log de Streamlit:
```
[DEBUG] format_docs_with_metadata] Recibidos 75 documentos  
[DEBUG] format_docs_with_metadata] Devolviendo 21636 caracteres de contexto  
[DEBUG] Despu√©s de invoke - answer_raw type: <class 'str'>, valor: []  ‚Üê ‚ùå VAC√çO
```

### B√∫squeda directa en FAISS:
```python
Documentos recuperados: 75

# Documento #2 (RELEVANTE):
"linajes, linaje ra, linaje linaje vix.
Okay. Son cuatro linajes. Y el otro
linaje eh Crick o Tri..."
```

‚úÖ **La informaci√≥n S√ç EXISTE en el √≠ndice**
‚ùå **Pero est√° FRAGMENTADA en chunks de 300 caracteres**

---

## ¬øPOR QU√â EL LLM RESPONDE `[]`?

Gemini 2.5 Pro es **muy estricto** con la precisi√≥n (temperature=0.4). Cuando recibe contexto fragmentado e incompleto:

1. **Ve fragmentos** como:
   - Chunk 1: "linajes, linaje ra, linaje linaje vix."
   - Chunk 2: "Okay. Son cuatro linajes. Y el otro"
   - Chunk 3: "linaje eh Crick o Tri"
   
2. **NO puede unir** la informaci√≥n dispersa en 75 chunks

3. **Decide que NO tiene suficiente informaci√≥n confiable**

4. **Responde `[]`** (vac√≠o) en lugar de arriesgarse a dar informaci√≥n incompleta

---

## SOLUCI√ìN INMEDIATA

### ‚ö° RE-INDEXAR CON chunk_size=800

**Ya est√° configurado en `reiniciar_indice.py`**, solo ejecutar:

```powershell
python reiniciar_indice.py
```

**Tiempo estimado:** 2-3 horas  
**Resultado esperado:**
- ‚úÖ Informaci√≥n completa en cada chunk
- ‚úÖ LLM puede responder con confianza
- ‚úÖ 90-95% de recall (vs 60-70% actual)

---

## WORKAROUND TEMPORAL (mientras re-indexas)

### Opci√≥n 1: Aumentar temperatura (NO recomendado)

```python
temperature=0.7  # M√°s "arriesgado", puede inventar
```

‚ùå **NO hacer esto** - GERARD requiere precisi√≥n quir√∫rgica

### Opci√≥n 2: Modificar prompt para aceptar respuestas parciales

Agregar al prompt:

```
Si la informaci√≥n est√° fragmentada o incompleta, proporciona lo que puedas 
bas√°ndote en los fragmentos disponibles, indicando claramente que la respuesta
es parcial.
```

‚ö†Ô∏è **Sub√≥ptimo** - mejor re-indexar

### Opci√≥n 3: Usar b√∫squeda h√≠brida (keyword + sem√°ntica)

‚ùå **Complejo** - requiere refactorizaci√≥n mayor

---

## PLAN DE ACCI√ìN

### ‚úÖ AHORA MISMO:

1. **RE-INDEXAR con chunk_size=800:**
   ```powershell
   python reiniciar_indice.py
   ```

2. **Esperar 2-3 horas** (protecciones anti-rate-limit activas)

3. **Editar `consultar_web.py`:**
   ```python
   k = 40  # Reducir de 75 a 40
   ```

4. **Reiniciar Streamlit**

5. **Probar de nuevo:** "INFORMACION SOBRE LINAJE RA, BIS, TRICK, JAC"

### ‚úÖ RESULTADO ESPERADO:

```json
[
  {
    "type": "normal",
    "content": "Los cuatro linajes son linaje RA, linaje BIS, linaje TRICK y linaje JAC... (Fuente: archivo.srt, Timestamp: 00:13:01)"
  }
]
```

---

## CONCLUSI√ìN

**El problema NO es:**
- ‚ùå La API key de Gemini
- ‚ùå El c√≥digo de Streamlit
- ‚ùå El retriever de FAISS
- ‚ùå El prompt

**El problema ES:**
- ‚úÖ **chunk_size=300 es DEMASIADO PEQUE√ëO**
- ‚úÖ **La informaci√≥n est√° FRAGMENTADA**
- ‚úÖ **El LLM NO puede unir 75 fragmentos peque√±os**

**La soluci√≥n ES:**
- ‚úÖ **RE-INDEXAR con chunk_size=800**
- ‚úÖ **Esperar 2-3 horas**
- ‚úÖ **Problema resuelto permanentemente**

---

**Creado:** 10 octubre 2025, 23:45  
**Prioridad:** üö® CR√çTICA  
**Acci√≥n requerida:** RE-INDEXAR AHORA
