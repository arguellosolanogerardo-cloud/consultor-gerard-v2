"""
Script optimizado para crear √≠ndice FAISS con rate limiting robusto.

VERSI√ìN MEJORADA que maneja:
- Rate limits de Google Embeddings API (sin cortes)
- Procesamiento en lotes con reintentos autom√°ticos
- Guardado incremental y checkpoints
- Capacidad de reanudar procesos interrumpidos

Uso:
    python ingestar_robusto.py              # Crear nuevo √≠ndice
    python ingestar_robusto.py --resume     # Reanudar proceso interrumpido
    python ingestar_robusto.py --force      # Forzar recreaci√≥n completa
"""

import os
import shutil
import argparse
from pathlib import Path
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from faiss_builder import FAISSVectorBuilder, BuilderConfig

# Cargar variables de entorno
load_dotenv()

# Verificar API Key
if not os.getenv("GOOGLE_API_KEY"):
    print("‚ùå ERROR: No se encontr√≥ GOOGLE_API_KEY en variables de entorno")
    print("   Configura la variable antes de ejecutar este script")
    exit(1)

# Configuraci√≥n de rutas
DATA_PATH = "documentos_srt/"
FAISS_INDEX_PATH = "faiss_index"
FAISS_INDEX_FILE = os.path.join(FAISS_INDEX_PATH, "index.faiss")
FAISS_PKL_FILE = os.path.join(FAISS_INDEX_PATH, "index.pkl")


def get_srt_documents(data_path):
    """
    Carga todos los archivos .srt del directorio especificado.
    
    Returns:
        list: Lista de documentos LangChain
    """
    print(f"\nüìÇ Cargando archivos .srt desde: {data_path}")
    
    if not os.path.exists(data_path):
        print(f"‚ùå ERROR: El directorio {data_path} no existe")
        return []
    
    documents = []
    file_count = 0
    
    # Cargar todos los archivos .srt
    for filename in os.listdir(data_path):
        if filename.endswith('.srt'):
            filepath = os.path.join(data_path, filename)
            try:
                loader = TextLoader(filepath, encoding='utf-8')
                docs = loader.load()
                
                # Agregar metadata con el nombre del archivo
                for doc in docs:
                    doc.metadata['source'] = filename
                
                documents.extend(docs)
                file_count += 1
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è Error al cargar {filename}: {e}")
    
    print(f"‚úÖ Cargados {file_count} archivos .srt ({len(documents)} documentos)")
    return documents


def split_documents_into_chunks(documents):
    """
    Divide los documentos en chunks m√°s peque√±os.
    
    Returns:
        list: Lista de chunks de documentos
    """
    print(f"\n‚úÇÔ∏è Dividiendo documentos en chunks...")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=10000,
        chunk_overlap=1000,
        length_function=len,
    )
    
    chunks = text_splitter.split_documents(documents)
    print(f"‚úÖ Creados {len(chunks)} chunks (size=10000, overlap=1000)")
    
    return chunks


def create_faiss_index(text_chunks, force_recreate=False, resume=False):
    """
    Crea √≠ndice FAISS usando el builder robusto con rate limiting.
    
    Args:
        text_chunks: Lista de chunks de documentos
        force_recreate: Si True, elimina √≠ndice existente
        resume: Si True, intenta reanudar desde checkpoint
    """
    # Verificar si ya existe
    if os.path.exists(FAISS_INDEX_FILE) and not force_recreate and not resume:
        print(f"\n‚úÖ √çndice FAISS ya existe en {FAISS_INDEX_PATH}")
        print(f"   Usa --force para recrear o --resume para continuar proceso interrumpido")
        return
    
    # Eliminar √≠ndice existente si se fuerza recreaci√≥n
    if force_recreate and os.path.exists(FAISS_INDEX_PATH):
        print(f"\nüóëÔ∏è Eliminando √≠ndice existente: {FAISS_INDEX_PATH}")
        shutil.rmtree(FAISS_INDEX_PATH)
    
    # Crear directorio
    os.makedirs(FAISS_INDEX_PATH, exist_ok=True)
    
    # Header
    print(f"\n{'='*70}")
    print(f"üöÄ CONSTRUCCI√ìN DE √çNDICE FAISS CON RATE LIMITING ROBUSTO")
    print(f"{'='*70}")
    print(f"üìÑ Total de chunks a procesar: {len(text_chunks)}")
    print(f"üîß Modelo embeddings: models/embedding-001")
    print(f"üìê Dimensi√≥n vectorial: 768")
    
    # Configuraci√≥n robusta
    config = BuilderConfig(
        rate_limit_per_minute=50,        # 50 req/min (conservador)
        batch_size=50,                   # Lotes de 50 docs
        save_every=500,                  # Guardar cada 500 vectores
        delay_between_requests=1.5,       # 1.5s entre lotes
        max_retries=5,                   # 5 reintentos
        initial_backoff=2,               # Backoff inicial 2s
        max_backoff=60,                  # Backoff m√°ximo 60s
        checkpoint_file='faiss_checkpoint.json'
    )
    
    print(f"\n‚öôÔ∏è CONFIGURACI√ìN:")
    print(f"   ‚Ä¢ Rate limit: {config.rate_limit_per_minute} peticiones/minuto")
    print(f"   ‚Ä¢ Batch size: {config.batch_size} documentos/lote")
    print(f"   ‚Ä¢ Guardar cada: {config.save_every} vectores")
    print(f"   ‚Ä¢ Delay entre lotes: {config.delay_between_requests}s")
    print(f"   ‚Ä¢ Reintentos m√°ximos: {config.max_retries}")
    print(f"   ‚Ä¢ Backoff exponencial: {config.initial_backoff}s ‚Üí {config.max_backoff}s")
    
    # Crear embeddings
    print(f"\nüîß Inicializando GoogleGenerativeAIEmbeddings...")
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    
    # Crear funci√≥n de embedding para el builder
    def embed_function(texts: list) -> list:
        """Wrapper para embeddings de Google"""
        return embeddings.embed_documents(texts)
    
    # Crear builder con funci√≥n de embedding
    builder = FAISSVectorBuilder(config, embed_function)
    
    try:
        print(f"\nüöÄ Iniciando construcci√≥n del √≠ndice...")
        if resume:
            print(f"‚ôªÔ∏è Modo RESUME: intentando reanudar desde checkpoint")
        
        # Construir √≠ndice (con progress bar, checkpoints, reintentos)
        index_file = os.path.join(FAISS_INDEX_PATH, "index.faiss")
        faiss_index = builder.build_from_documents(
            documents=text_chunks,
            output_path=index_file,
            resume_from_checkpoint=resume
        )
        
        # IMPORTANTE: Crear vectorstore de LangChain para compatibilidad
        print(f"\nüîß Creando vectorstore compatible con LangChain...")
        from langchain_community.vectorstores import FAISS
        from langchain_community.docstore.in_memory import InMemoryDocstore
        
        # Crear docstore con los documentos
        docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(text_chunks)})
        
        # Crear mapping de √≠ndice a doc_id
        index_to_docstore_id = {i: str(i) for i in range(len(text_chunks))}
        
        # Crear vectorstore de LangChain
        vectorstore = FAISS(
            embedding_function=embeddings,
            index=faiss_index,
            docstore=docstore,
            index_to_docstore_id=index_to_docstore_id
        )
        
        # Guardar con LangChain (formato compatible)
        print(f"üíæ Guardando vectorstore en formato LangChain...")
        vectorstore.save_local(FAISS_INDEX_PATH)
        print(f"‚úÖ Vectorstore guardado correctamente")
        
        # √âxito - mostrar resumen
        print(f"\n{'='*70}")
        print(f"‚úÖ √çNDICE FAISS CREADO EXITOSAMENTE")
        print(f"{'='*70}")
        print(f"üìÅ Ubicaci√≥n: {FAISS_INDEX_PATH}")
        print(f"üìÑ Archivos:")
        print(f"   ‚Ä¢ {FAISS_INDEX_FILE}")
        print(f"   ‚Ä¢ {FAISS_PKL_FILE}")
        
        # Mostrar tama√±os
        if os.path.exists(FAISS_INDEX_FILE) and os.path.exists(FAISS_PKL_FILE):
            size_faiss = os.path.getsize(FAISS_INDEX_FILE) / (1024*1024)
            size_pkl = os.path.getsize(FAISS_PKL_FILE) / (1024*1024)
            print(f"\nüìä Tama√±os:")
            print(f"   ‚Ä¢ index.faiss: {size_faiss:.2f} MB")
            print(f"   ‚Ä¢ index.pkl: {size_pkl:.2f} MB")
            print(f"   ‚Ä¢ Total: {size_faiss + size_pkl:.2f} MB")
        
        print(f"\nüí° SIGUIENTE PASO:")
        print(f"   Ejecuta consultar_web.py o consultar_terminal.py para usar el √≠ndice")
        
    except KeyboardInterrupt:
        print(f"\n\n‚ö†Ô∏è PROCESO INTERRUMPIDO POR USUARIO (Ctrl+C)")
        print(f"üíæ Progreso guardado en checkpoint")
        print(f"üîÑ Para continuar donde qued√≥, ejecuta:")
        print(f"   python ingestar_robusto.py --resume")
        
    except Exception as e:
        print(f"\n\n‚ùå ERROR DURANTE LA CONSTRUCCI√ìN:")
        print(f"   {str(e)}")
        print(f"üíæ Progreso guardado en checkpoint")
        print(f"üîÑ Para reintentar desde donde fall√≥, ejecuta:")
        print(f"   python ingestar_robusto.py --resume")
        raise


def main():
    """Funci√≥n principal"""
    parser = argparse.ArgumentParser(
        description="Crea √≠ndice FAISS robusto con rate limiting para archivos .srt"
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="Fuerza la recreaci√≥n del √≠ndice (elimina el existente)"
    )
    parser.add_argument(
        "--resume",
        action="store_true",
        help="Reanuda proceso interrumpido desde √∫ltimo checkpoint"
    )
    
    args = parser.parse_args()
    
    # Banner inicial
    print(f"\n{'='*70}")
    print(f"üìö INGESTI√ìN DE DOCUMENTOS .SRT ‚Üí √çNDICE FAISS")
    print(f"{'='*70}")
    
    # Paso 1: Cargar documentos
    documents = get_srt_documents(DATA_PATH)
    if not documents:
        print(f"\n‚ùå No se encontraron documentos para procesar")
        return
    
    # Paso 2: Dividir en chunks
    text_chunks = split_documents_into_chunks(documents)
    if not text_chunks:
        print(f"\n‚ùå No se generaron chunks para procesar")
        return
    
    # Paso 3: Crear √≠ndice FAISS con builder robusto
    create_faiss_index(
        text_chunks=text_chunks,
        force_recreate=args.force,
        resume=args.resume
    )
    
    print(f"\n{'='*70}")
    print(f"‚úÖ PROCESO COMPLETADO")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    main()
