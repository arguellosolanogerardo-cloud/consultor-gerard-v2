import os
import json
import re
import colorama
import argparse
import threading
import itertools
import sys
import time
import getpass
import uuid
from dotenv import load_dotenv
import keyring
from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from datetime import datetime

# Inicializamos colorama para que los colores funcionen en todas las terminales
colorama.init(autoreset=True)

# --- Carga la API Key ---
# Nota: no inicializamos la API ni recursos de red al importar el mÃ³dulo.
# Creamos una funciÃ³n para construir la cadena de recuperaciÃ³n (llm + vectorstore)
def build_retrieval_chain(api_key: str):
    """Construye y devuelve el retrieval_chain usando la API key proporcionada.

    Carga el Ã­ndice FAISS persistido en `faiss_index/`.
    """
    # Small helper to run blocking calls in a thread and show a spinner in console
    def run_with_spinner(func, *args, message="Procesando..."):
        result_holder = {}

        def target():
            try:
                result_holder['result'] = func(*args)
            except Exception as e:
                result_holder['error'] = e

        thread = threading.Thread(target=target)
        thread.start()

        spinner = itertools.cycle(['|', '/', '-', '\\'])
        sys.stdout.write(message + ' ')
        sys.stdout.flush()
        try:
            while thread.is_alive():
                sys.stdout.write(next(spinner))
                sys.stdout.flush()
                time.sleep(0.1)
                sys.stdout.write('\b')
        except KeyboardInterrupt:
            pass
        thread.join()

        sys.stdout.write('\r' + ' ' * (len(message) + 2) + '\r')

        if 'error' in result_holder:
            raise result_holder['error']
        return result_holder.get('result')

    # Load LLM and embeddings with spinner to give feedback for slow init
    llm = run_with_spinner(lambda: GoogleGenerativeAI(model="models/gemini-2.5-pro", google_api_key=api_key), message="Inicializando LLM...")
    embeddings = run_with_spinner(lambda: GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key), message="Inicializando embeddings...")

    try:
        vectorstore = run_with_spinner(lambda: FAISS.load_local(folder_path="faiss_index", embeddings=embeddings, allow_dangerous_deserialization=True), message="Cargando Ã­ndice FAISS (puede tardar)...")
    except Exception as e:
        print(f"Error cargando FAISS index: {e}")
        raise

    retriever = vectorstore.as_retriever()
    retrieval_chain = (
        {
            "context": (lambda x: x["input"]) | retriever | format_docs_with_metadata,
            "input": (lambda x: x["input"]) 
        }
        | prompt
        | llm
        | StrOutputParser()
    )

    return retrieval_chain


def get_api_key():
    """Intentar obtener la API key de varias fuentes en orden:
    1. keyring del sistema (servicio 'consultor-gerard', nombre 'google_api_key')
    2. variable de entorno GOOGLE_API_KEY
    3. archivo .env (load_dotenv ya se usa en main)
    Devuelve la cadena o None si no se encuentra.
    """
    # 1) keyring
    try:
        kr = keyring.get_password('consultor-gerard', 'google_api_key')
        if kr:
            return kr
    except Exception:
        # Si keyring falla por cualquier motivo, lo ignoramos y continuamos
        pass

    # 2) environment
    api = os.environ.get('GOOGLE_API_KEY')
    if api:
        return api

    # 3) .env ya cargado por caller
    return None

# --- PERSONALIDAD DE "GERARD" (CON PROMPT CORREGIDO) ---
prompt = ChatPromptTemplate.from_template(r"""
ğŸ”¬ GERARD v3.01 - Sistema de AnÃ¡lisis Investigativo Avanzado
IDENTIDAD DEL SISTEMA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Nombre: GERARD
VersiÃ³n: 3.01 - Analista Forense Documental
Modelo Base: Gemini Pro Latest 2.5
Temperatura: 0.2-0.3 (MÃ¡xima PrecisiÃ³n y Consistencia)
EspecializaciÃ³n: CriptoanÃ¡lisis de Archivos .srt
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
MISIÃ“N CRÃTICA
Eres GERARD, un sistema de inteligencia analÃ­tica especializado en arqueologÃ­a documental de archivos de subtÃ­tulos (.srt). Tu propÃ³sito es descubrir patrones ocultos, mensajes encriptados y conexiones invisibles que emergen al correlacionar mÃºltiples documentos mediante tÃ©cnicas forenses avanzadas.
ConfiguraciÃ³n de Temperatura Optimizada (0.2-0.3)
Esta temperatura baja garantiza:
â€¢ Consistencia absoluta entre consultas repetidas
â€¢ Reproducibilidad de hallazgos para verificaciÃ³n
â€¢ PrecisiÃ³n quirÃºrgica en extracciÃ³n de datos
â€¢ EliminaciÃ³n de variabilidad en respuestas crÃ­ticas
â€¢ Confiabilidad forense en anÃ¡lisis investigativos
________________________________________
ğŸš¨ PROTOCOLOS DE SEGURIDAD ANALÃTICA
REGLAS ABSOLUTAS (Nivel de Cumplimiento: 100%)
ğŸ”´ PROHIBICIÃ“N NIVEL 1: FABRICACIÃ“N DE DATOS
â”œâ”€ âŒ NO inventar informaciÃ³n bajo ninguna circunstancia
â”œâ”€ âŒ NO usar conocimiento del modelo base (entrenamiento general)
â”œâ”€ âŒ NO suponer o inferir mÃ¡s allÃ¡ de lo textualmente disponible
â””â”€ âŒ NO completar informaciÃ³n faltante con lÃ³gica externa

ğŸ”´ PROHIBICIÃ“N NIVEL 2: CONTAMINACIÃ“N ANALÃTICA
â”œâ”€ âŒ NO mezclar anÃ¡lisis con citas textuales
â”œâ”€ âŒ NO parafrasear cuando se requiere texto literal
â”œâ”€ âŒ NO interpretar sin declarar explÃ­citamente que es interpretaciÃ³n
â””â”€ âŒ NO omitir informaciÃ³n contradictoria si existe

ğŸŸ¢ MANDATOS OBLIGATORIOS
â”œâ”€ âœ… Cada afirmaciÃ³n DEBE tener cita textual verificable
â”œâ”€ âœ… Cada cita DEBE incluir: [Documento] + [Timestamp] + [Texto Literal]
â”œâ”€ âœ… Cada anÃ¡lisis DEBE separarse claramente de evidencias
â”œâ”€ âœ… Cada consulta DEBE ejecutar los 8 Protocolos de BÃºsqueda Profunda
â””â”€ âœ… Cada respuesta DEBE incluir nivel de confianza estadÃ­stico
________________________________________
ğŸ” SISTEMA DE ANÃLISIS MULTINIVEL
NIVEL 1: EXTRACCIÃ“N SUPERFICIAL (Baseline)
Objetivo: Captura literal de informaciÃ³n explÃ­cita
TÃ©cnica: Lectura directa y indexaciÃ³n
Profundidad: 0-20% del contenido oculto
NIVEL 2: ANÃLISIS CORRELACIONAL (Intermediate)
Objetivo: ConexiÃ³n de fragmentos dispersos
TÃ©cnicas:
    â”œâ”€ Mapeo de relaciones temÃ¡ticas
    â”œâ”€ DetecciÃ³n de patrones recurrentes
    â”œâ”€ IdentificaciÃ³n de complementariedades
    â”œâ”€ TriangulaciÃ³n de fuentes mÃºltiples
    â””â”€ ConstrucciÃ³n de narrativas coherentes
Profundidad: 20-50% del contenido oculto
NIVEL 3: CRIPTOANÃLISIS FORENSE (Advanced)
Objetivo: Descubrimiento de mensajes encriptados
Profundidad: 50-85% del contenido oculto
________________________________________
ğŸ” PROTOCOLOS DE BÃšSQUEDA PROFUNDA (8 CHECKS OBLIGATORIOS)
CHECK #1: ANÃLISIS ACRÃ“STICO MULTINIVEL
MÃ‰TODO: ... (ejecutar los pasos descritos en el protocolo suministrado)
CHECK #2: ANÃLISIS DE PATRONES NUMÃ‰RICOS
MÃ‰TODO: ...
CHECK #3: ANÃLISIS DE PALABRAS CLAVE DISTRIBUIDAS
MÃ‰TODO: ...
CHECK #4: ANÃLISIS SECUENCIAL CRONOLÃ“GICO
MÃ‰TODO: ...
CHECK #5: ANÃLISIS CONTEXTUAL DE FRAGMENTACIÃ“N
MÃ‰TODO: ...
CHECK #6: ANÃLISIS DE ANOMALÃAS Y REPETICIONES
MÃ‰TODO: ...
CHECK #7: ANÃLISIS DE OMISIONES DELIBERADAS
MÃ‰TODO: ...
CHECK #8: ANÃLISIS DE METADATOS Y MARCADORES OCULTOS
MÃ‰TODO: ...
________________________________________
ğŸ“‹ ESTRUCTURA DE RESPUESTA OPTIMIZADA PARA TEMP 0.2-0.3
FORMATO ESTANDARIZADO (Reproducibilidad Garantizada)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ”¬ ANÃLISIS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Timestamp de AnÃ¡lisis: [{date}]
Consulta Procesada: "{input}"
Temperatura Operativa: 0.2-0.3
Hash de SesiÃ³n: [{session_hash}]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECCIÃ“N 1: SÃNTESIS INVESTIGATIVA
[Resuma hallazgos y evidencias; siga estrictamente las reglas de cita y separaciÃ³n de evidencia/interpretaciÃ³n]
SECCIÃ“N 2: EVIDENCIA FORENSE ESTRUCTURADA
[Agrupe por documento y cite por timestamp: siempre texto literal]
SECCIÃ“N 3: ÃNDICE DE FUENTES Y MAPEO
[Reporte de cobertura y relevancia]
SECCIÃ“N 4: METADATOS Y GARANTÃA DE CALIDAD
[Reporte de ejecuciÃ³n de checks y nivel de confianza]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
FIN DEL ANÃLISIS

BasÃ¡ndote estrictamente en el contenido disponible en el contexto (no accedas a fuentes externas), responde la consulta del usuario respetando todas las prohibiciones y mandatos arriba definidos.
""")

# --- FUNCIÃ“N PARA FORMATEAR DOCUMENTOS (CON LIMPIEZA REFORZADA) ---
def get_cleaning_pattern():
    """Crea un patrÃ³n de regex robusto para eliminar textos no deseados."""
    texts_to_remove = [
        '[Spanish (auto-generated)]',
        '[DownSub.com]',
        '[MÃºsica]',
        '[Aplausos]'
    ]
    # Este patrÃ³n es mÃ¡s robusto: busca el texto dentro de los corchetes,
    # permitiendo espacios en blanco opcionales alrededor.
    robust_patterns = [r'\[\s*' + re.escape(text[1:-1]) + r'\s*\]' for text in texts_to_remove]
    return re.compile(r'|'.join(robust_patterns), re.IGNORECASE)

cleaning_pattern = get_cleaning_pattern()

def format_docs_with_metadata(docs):
    """Prepara los documentos recuperados, limpiando robustamente el contenido y los timestamps."""
    formatted_strings = []
    for doc in docs:
        source_filename = os.path.basename(doc.metadata.get('source', 'Fuente desconocida'))
        
        # Eliminar extensiÃ³n .srt para fuentes mÃ¡s limpias
        if source_filename.endswith('.srt'):
            source_filename = source_filename[:-4]
        
        # 1. Limpieza de textos no deseados
        cleaned_content = cleaning_pattern.sub('', doc.page_content)

        # 2. Â¡NUEVO! Eliminar milisegundos de los timestamps
        # El patrÃ³n busca HH:MM:SS,ms y lo reemplaza con HH:MM:SS
        cleaned_content = re.sub(r'(\d{2}:\d{2}:\d{2}),\d{3}', r'\1', cleaned_content)
        
        # 3. Limpieza de lÃ­neas vacÃ­as
        cleaned_content = "\n".join(line for line in cleaned_content.split('\n') if line.strip())
        
        if cleaned_content:
            formatted_strings.append(f"Fuente del Archivo: {source_filename}\nContenido:\n{cleaned_content}")
            
    return "\n\n---\n\n".join(formatted_strings)

# --- Cadena de recuperaciÃ³n (LCEL) ---
# ... el retrieval_chain se construye con `build_retrieval_chain(api_key)` cuando
# se ejecute el script como programa principal.

# --- NUEVA FUNCIÃ“N para convertir el JSON a texto plano para el log ---
def get_clean_text_from_json(json_string):
    """Convierte la respuesta JSON en una cadena de texto simple y legible."""
    try:
        match = re.search(r'\[.*\]', json_string, re.DOTALL)
        if not match:
            return json_string

        data = json.loads(match.group(0))
        full_text = "".join([item.get("content", "") for item in data])
        return full_text
    except Exception:
        return json_string

# --- NUEVA FUNCIÃ“N para guardar la conversaciÃ³n en un archivo ---
def save_to_log(question, user, answer_json):
    """Guarda la pregunta y la respuesta en un archivo de registro."""
    clean_answer = get_clean_text_from_json(answer_json)
    timestamp = datetime.now().strftime("%Y-%m-%d %H:M:%S")
    
    with open("gerard_log.txt", "a", encoding="utf-8") as f:
        f.write(f"--- ConversaciÃ³n del {timestamp} ---\n")
        f.write(f"Usuario: {user}\n")
        f.write(f"Pregunta: {question}\n")
        f.write(f"Respuesta de GERARD: {clean_answer}\n")
        f.write("="*40 + "\n\n")

# --- FUNCIÃ“N PARA IMPRIMIR LA RESPUESTA CON MÃšLTIPLES COLORES ---
def print_json_answer(json_string):
    # Failsafe: volvemos a limpiar la respuesta final por si acaso
    cleaned_string = cleaning_pattern.sub('', json_string)
    
    try:
        match = re.search(r'\[.*\]', cleaned_string, re.DOTALL)
        if not match:
            print(f"{colorama.Fore.RED}Respuesta no es un JSON vÃ¡lido:\n{cleaned_string}")
            return

        data = json.loads(match.group(0))
        
        for item in data:
            content_type = item.get("type", "normal")
            content = item.get("content", "")
            
            if content_type == "emphasis":
                print(f"{colorama.Fore.YELLOW}{content}", end="")
            else:
                parts = re.split(r'(\(.*?\))', content)
                for part in parts:
                    if part.startswith('(') and part.endswith(')'):
                        print(f"{colorama.Fore.MAGENTA}{part}", end="")
                    else:
                        print(f"{colorama.Style.RESET_ALL}{part}", end="")
        print()
    except json.JSONDecodeError:
        print(f"{colorama.Fore.RED}Error: El modelo no devolviÃ³ un JSON vÃ¡lido. Respuesta recibida:\n{cleaned_string}")
    except Exception as e:
        print(f"{colorama.Fore.RED}OcurriÃ³ un error inesperado al procesar la respuesta: {e}")

# --- Bucle de InteracciÃ³n ---
def main():
    """FunciÃ³n principal que lanza el loop interactivo. Protegida para que no se ejecute al importar."""
    parser = argparse.ArgumentParser()
    parser.add_argument("--no-store", action="store_true", help="No almacenar la API key en keyring aunque se provea interactiva")
    args = parser.parse_args()

    load_dotenv()

    # Intentar obtener la key desde keyring o entornos
    api_key = get_api_key()
    if not api_key:
        # Pedir interactivamente la clave al usuario
        print("No se encontrÃ³ GOOGLE_API_KEY en keyring/entorno. Puedes pegar tu clave ahora (se ocultarÃ¡).")
        try:
            entered = getpass.getpass(prompt="Introduce tu GOOGLE_API_KEY (vacÃ­o para cancelar): ")
        except Exception:
            entered = input("Introduce tu GOOGLE_API_KEY (vacÃ­o para cancelar): ")

        if not entered:
            print("No se proporcionÃ³ clave. Abortando.")
            return

        api_key = entered.strip()
        # Ofrecer guardar en keyring
        if not args.no_store:
            try:
                save_choice = input("Â¿Deseas guardar esta clave en el keyring del sistema para futuras ejecuciones? [y/N]: ")
                if save_choice.lower().startswith('y'):
                    try:
                        keyring.set_password('consultor-gerard', 'google_api_key', api_key)
                        print("Clave guardada en keyring con servicio 'consultor-gerard'.")
                    except Exception as e:
                        print(f"No se pudo guardar en keyring: {e}")
            except Exception:
                # No crÃ­tico si falla la interacciÃ³n para guardar
                pass

    # Construir la cadena de recuperaciÃ³n real
    try:
        retrieval_chain = build_retrieval_chain(api_key)
    except Exception as e:
        print(f"Error inicializando el pipeline real: {e}")
        return

    print("GERARD listo. Escribe tu pregunta o 'salir' para terminar.")
    user_name = input("Por favor, introduce tu nombre para comenzar: ")

    while True:
        prompt_text = f"\nTu pregunta {colorama.Fore.BLUE}{user_name.upper()}{colorama.Style.RESET_ALL}: "
        pregunta = input(prompt_text)

        if pregunta.lower() == 'salir':
            break

        print("Buscando...")
        try:
            ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            session_hash = str(uuid.uuid4())
            answer = retrieval_chain.invoke({"input": pregunta, "date": ts, "session_hash": session_hash})
            print("\nRespuesta de GERARD:")
            print_json_answer(answer)
            save_to_log(pregunta, user_name.upper(), answer)

        except Exception as e:
            print(f"\n{colorama.Fore.RED}OcurriÃ³ un error al procesar tu pregunta: {e}")


if __name__ == "__main__":
    main()

